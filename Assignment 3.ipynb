{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Prediction/Modeling\n",
    "Due: Friday, Nov 22, 2019 in class\n",
    "\n",
    "Submission: Complete this notebook and print out the output or electronically submit it.\n",
    "\n",
    "Everything you need to complete is marked with a TODO. For textual questions create a new cell under the question to respond to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Game of Thrones is one of the most watched TV series of all times.  With hundreds of characters and more than 22K sentences, this dataset aims to help you test your text mining skills. The content is pretty simple: the dataset contains each and every sentence said in the serie together with who has said it, the episode and the season. For the time being the dataset includes episodes from Season 1 to Season 7. You can download the dataset here: https://github.com/sjyk/cmsc21800/blob/master/got.csv\n",
    "\n",
    "### Loading the Dataset\n",
    "The first task is to load the dataset into a pandas dataframe and filter relevant rows for this assignment. We only care about the rows for chracters that are present in all 7 of the seasons and speak a sufficient amount. Filter the rows to include only those with the speaker name \"Cersei\", \"Daenerys\", \"Tyrion\", and \"Arya\"--make sure you handle upper-case and lower case properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_dataset(filename):\n",
    "    '''TODO: Given a filename return a dataframe \n",
    "       containing the rows.\n",
    "       \n",
    "       Only return those rows with a name:\n",
    "       * \"cersei\"\n",
    "       * \"daenerys\"\n",
    "       * \"tyrion\"\n",
    "       * \"arya\"\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_csv(filename, delimiter=';')\n",
    "    return df[ (df['Name'] == 'cersei') | \\\n",
    "               (df['Name'] == 'tyrion') | \\\n",
    "               (df['Name'] == 'daenerys') | \\\n",
    "               (df['Name'] == 'arya')]\n",
    "\n",
    "    #raise ValueError(\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cluster Analysis\n",
    "Next, we will mine this dataset to understand what types of structure exist. In the next task, we will write a featurizer that takes the dataset and converts it into a set of feature vectors. We will use a tf-idf featurizer to do this:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def featurize(quotes):\n",
    "    '''TODO: takes a set of quotes as input and returns two things: an array of feature vectors \n",
    "             and the featurizer. \n",
    "             \n",
    "       * Use the tfidfvectorizer from sklearn and remove english stopwords and restrict the features to \n",
    "             words that appear in *at most* 20 quotes.\n",
    "    \n",
    "       Return values (returns a tuple!!): \n",
    "                      X - a dense numpy array of feature vectors representing the text data.\n",
    "                      vectorizer - a TfidfVectorizer object.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=20)\n",
    "    X = vectorizer.fit_transform(quotes)\n",
    "    return X.todense(), vectorizer\n",
    "\n",
    "    #raise ValueError('Not Implemented')\n",
    "    \n",
    "\n",
    "df = load_dataset('got.csv')\n",
    "X, vectorizer = featurize(df['Sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the principal components of this featurized dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def compute_pca(features, components=2):\n",
    "    '''TODO: Calculate the first two principal components of the \n",
    "             features that you have. Return the components, \n",
    "             the explained variance, and N-D representation of the\n",
    "             feature vectors.\n",
    "       \n",
    "       Return Values (returns a 4-tuple): \n",
    "              * axes (the principal components from .components_)\n",
    "              * Y (the dimensionality reduced data)\n",
    "              * c = explained variance on a range from [0,1]\n",
    "    '''\n",
    "    \n",
    "    pca = PCA(n_components=components) #find 2 principal components\n",
    "    Y = pca.fit_transform(X)\n",
    "\n",
    "    c = np.sum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    return pca.components_, Y, c\n",
    "    #raise ValueError(\"Not Implemented\")\n",
    "\n",
    "#compute PCA\n",
    "pcs, Y, c = compute_pca(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code to interpret the PCA components. Write a function that uses the vectorize to determine the words whose presence or absence is strongest in the PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC 1: Most Positive:  {'send', 'isnt', 'choice', 'prince', 'ship', 'podrick', 'game', 'talk', 'shae'}\n",
      "\n",
      "PC 2: Most Positive:  {'ones', 'care', 'drogo', 'sleep', 'promised', 'fine', 'khal', 'havent', 'mean'}\n",
      "\n",
      "PC 3: Most Positive:  {'sleep', 'promised', 'jorah', 'prince', 'feel', 'kind', 'oberyn', 'save', 'coming', 'whore'}\n",
      "\n",
      "Explained Variance:  0.008340856769294209\n"
     ]
    }
   ],
   "source": [
    "def top_k(pc,vectorizer,k=10):\n",
    "    '''TODO: Finds the highest (most positive) weighted elements in a pc and \n",
    "             then returns the words that correspond to those elements. \n",
    "             \n",
    "             Exclude all words that are less than 3 letters.\n",
    "             \n",
    "       Return Value: A set of k words\n",
    "    '''\n",
    "    \n",
    "    weights = list(zip(pc, vectorizer.get_feature_names()))\n",
    "    weights.sort()\n",
    "    return set([word for _, word in weights[:k] if len(word) > 3])\n",
    "    #raise ValueError(\"Not Implemented\")\n",
    "\n",
    "#extract each of the pcs\n",
    "pc1, pc2, pc3 = pcs\n",
    "    \n",
    "print(\"PC 1: Most Positive: \", top_k(pc1, vectorizer))\n",
    "print()\n",
    "print(\"PC 2: Most Positive: \", top_k(pc2, vectorizer))\n",
    "print()\n",
    "print(\"PC 3: Most Positive: \", top_k(pc3, vectorizer))\n",
    "print()\n",
    "print(\"Explained Variance: \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you who know the story, you can see the story arcs in the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting The Speaker\n",
    "Now, we will have you predict the speaker from the patterns in the text. The first step is to define a training and a test set. Write the following function that splits the loaded dataset into a training set (80% of the data) and a test set (20% of the data). The partition should be random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe):\n",
    "    '''TODO: Write a function that splits the dataset into a \n",
    "             training set and a testing set\n",
    "        \n",
    "       Return values (returns a tuple!) : - A training set 80% of the data, - A test set 20% of the data.\n",
    "    '''\n",
    "    mask = (np.random.rand(len(dataframe)) <= 0.8)\n",
    "    \n",
    "    return df[mask], df[~mask]\n",
    "\n",
    "train, test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO. Your task is to build a classifier that will achieve at least 45% accuracy on this dataset\n",
    "To achieve this you will have to manipulate the data and play around with different featurization techniques and modeling choices. First, write a function that \"fits\" a language model, such as TFIDF, to the training dataset. It is up to you to tune the parameters for the vectorizer you choose approriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model(training_quotes):\n",
    "    '''TODO: Write a function that instantiates a vectorizer (e.g., a TfidfVectorizer), runs fit() and \n",
    "       returns the vectorizer.\n",
    "    '''\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english',max_df=30)\n",
    "    vectorizer.fit(train['Sentence'])\n",
    "    \n",
    "    return vectorizer    \n",
    "    \n",
    "    #raise ValueError(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will write a featurizer that takes in a set of quotes and returns an array of feature vectors using the language model above. You may add whatever additional features you find useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_featurize(quotes, vectorizer):\n",
    "    X = vectorizer.transform(quotes)\n",
    "    return X.todense()\n",
    "\n",
    "    #raise ValueError(\"Not Implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, determine the right machine learning model to use to actually make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanjaykrishnan/Documents/cmsc21800/venv/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        arya       0.42      0.26      0.32       144\n",
      "      cersei       0.44      0.29      0.35       188\n",
      "    daenerys       0.38      0.26      0.31       162\n",
      "      tyrion       0.40      0.65      0.50       282\n",
      "\n",
      "    accuracy                           0.41       776\n",
      "   macro avg       0.41      0.36      0.37       776\n",
      "weighted avg       0.41      0.41      0.39       776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = language_model(train['Sentence'])\n",
    "X = prediction_featurize(train['Sentence'], vectorizer)\n",
    "Y = train['Name']\n",
    "\n",
    "Xtest = prediction_featurize(test['Sentence'], vectorizer)\n",
    "Ytest = test['Name']\n",
    "\n",
    "##TODO: YOUR CODE HERE (Bad Example That Doesn't Work Well)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X,Y)\n",
    "pred = clf.predict(Xtest)\n",
    "##END:TODO\n",
    "\n",
    "\n",
    "#calculate accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Ytest, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
